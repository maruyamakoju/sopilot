"""Insurance MVP - Streamlit Dashboard for PoC Demo.

Interactive dashboard for Sompo Japan PoC demonstration.
Supports video upload, pipeline execution, and results visualization.
Backend modes: Mock (no GPU), Real (Qwen2.5-VL-7B on GPU), Replay (load JSON).

Usage:
    streamlit run insurance_mvp/dashboard.py
"""

from __future__ import annotations

import json
import sys
import tempfile
import time
from datetime import datetime
from pathlib import Path

import streamlit as st

# Ensure project root is on path
PROJECT_ROOT = Path(__file__).resolve().parent.parent
if str(PROJECT_ROOT) not in sys.path:
    sys.path.insert(0, str(PROJECT_ROOT))

from insurance_mvp.config import CosmosBackend, PipelineConfig  # noqa: E402
from insurance_mvp.pipeline import InsurancePipeline  # noqa: E402

# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------

DEMO_DIR = PROJECT_ROOT / "data" / "dashcam_demo"
SEVERITY_COLORS = {
    "HIGH": "#DC2626",
    "MEDIUM": "#F59E0B",
    "LOW": "#3B82F6",
    "NONE": "#10B981",
}
SEVERITY_EMOJI = {"HIGH": "üî¥", "MEDIUM": "üü°", "LOW": "üîµ", "NONE": "üü¢"}
PRIORITY_COLORS = {
    "URGENT": "#DC2626",
    "STANDARD": "#F59E0B",
    "LOW_PRIORITY": "#10B981",
}

# ---------------------------------------------------------------------------
# Translations
# ---------------------------------------------------------------------------

TRANSLATIONS = {
    "en": {
        "title": "Insurance Claim Assessment",
        "subtitle": "AI-Powered Dashcam Video Analysis",
        "sidebar_title": "Settings",
        "lang_label": "Language",
        "mode_label": "Input Mode",
        "mode_demo": "Demo Videos",
        "mode_upload": "Upload Video",
        "select_video": "Select Demo Video",
        "upload_prompt": "Upload a dashcam video (.mp4)",
        "run_button": "Run Assessment",
        "running": "Processing video...",
        "results_title": "Assessment Results",
        "severity": "Severity",
        "confidence": "Confidence",
        "prediction_set": "Prediction Set (90% CI)",
        "review_priority": "Review Priority",
        "fault_title": "Fault Assessment",
        "fault_ratio": "Fault Ratio",
        "scenario": "Scenario Type",
        "reasoning": "Reasoning",
        "rules": "Applicable Rules",
        "fraud_title": "Fraud Detection",
        "fraud_score": "Fraud Risk Score",
        "fraud_indicators": "Indicators",
        "fraud_reasoning": "Reasoning",
        "evidence_title": "Evidence",
        "hazards_title": "Detected Hazards",
        "causal": "Causal Analysis",
        "action": "Recommended Action",
        "processing_time": "Processing Time",
        "stage_timings": "Stage Timings",
        "report_title": "Report",
        "download_json": "Download JSON",
        "download_html": "Download HTML Report",
        "no_results": "No assessment results. Run the pipeline first.",
        "pipeline_success": "Pipeline completed successfully!",
        "pipeline_failed": "Pipeline failed",
        "ground_truth": "Ground Truth",
        "gt_match": "Match",
        "gt_mismatch": "Mismatch",
        "ego_vehicle": "Ego Vehicle",
        "other_vehicle": "Other Vehicle",
        "shared": "Shared Fault",
        "vlm_backend": "VLM Backend",
        "conformal_alpha": "Conformal Alpha",
        "overview": "Overview",
        "details": "Details",
        "comparison": "Ground Truth Comparison",
        "backend_label": "Backend",
        "backend_mock": "Mock (No GPU)",
        "backend_real": "Real (Qwen2.5-VL-7B)",
        "backend_replay": "Replay (Load JSON)",
        "replay_select": "Select Result File",
        "replay_upload": "Or upload JSON file",
        "replay_loaded": "Results loaded from file",
        "replay_no_files": "No result files found in reports/",
        "gpu_not_available": "GPU not available. Install PyTorch with CUDA support.",
        "gpu_info": "GPU",
        "real_warning": "Real VLM inference requires GPU (14GB+ VRAM). ~2-3 min per video.",
        "replay_source": "Source",
        "mode_batch": "Batch Results",
        "batch_title": "Batch Processing Results",
        "batch_upload_prompt": "Upload batch_report.json",
        "batch_loaded": "Batch report loaded",
        "batch_total_videos": "Total Videos",
        "batch_successful": "Successful",
        "batch_failed": "Failed",
        "batch_severity_dist": "Severity Distribution",
        "batch_avg_confidence": "Avg Confidence",
        "batch_avg_time": "Avg Time / Video",
        "batch_total_time": "Total Processing Time",
        "batch_video_id": "Video ID",
        "batch_file": "File",
        "batch_severity": "Severity",
        "batch_confidence": "Confidence",
        "batch_processing_time": "Time (s)",
        "batch_status": "Status",
        "batch_no_report": "No batch report loaded. Upload a batch_report.json file.",
        "batch_per_video": "Per-Video Results",
        "batch_aggregate": "Aggregate Statistics",
        "accuracy_label": "Severity Accuracy",
        "accuracy_note": "9/10 on expanded 10-video test suite",
    },
    "ja": {
        "title": "‰øùÈô∫ÈáëË´ãÊ±ÇÊüªÂÆö",
        "subtitle": "AIÊê≠Ëºâ„Éâ„É©„É¨„Ç≥Êò†ÂÉèËß£Êûê",
        "sidebar_title": "Ë®≠ÂÆö",
        "lang_label": "Ë®ÄË™û",
        "mode_label": "ÂÖ•Âäõ„É¢„Éº„Éâ",
        "mode_demo": "„Éá„É¢ÂãïÁîª",
        "mode_upload": "ÂãïÁîª„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ",
        "select_video": "„Éá„É¢ÂãïÁîª„ÇíÈÅ∏Êäû",
        "upload_prompt": "„Éâ„É©„É¨„Ç≥ÂãïÁîª„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ (.mp4)",
        "run_button": "ÊüªÂÆöÂÆüË°å",
        "running": "ÂãïÁîª„ÇíÂá¶ÁêÜ‰∏≠...",
        "results_title": "ÊüªÂÆöÁµêÊûú",
        "severity": "ÈáçÂ§ßÂ∫¶",
        "confidence": "Á¢∫‰ø°Â∫¶",
        "prediction_set": "‰∫àÊ∏¨„Çª„ÉÉ„Éà (90% CI)",
        "review_priority": "ÂØ©ÊüªÂÑ™ÂÖàÂ∫¶",
        "fault_title": "ÈÅéÂ§±Ââ≤Âêà",
        "fault_ratio": "ÈÅéÂ§±Ââ≤Âêà",
        "scenario": "„Ç∑„Éä„É™„Ç™„Çø„Ç§„Éó",
        "reasoning": "Âà§Êñ≠Ê†πÊã†",
        "rules": "ÈÅ©Áî®„É´„Éº„É´",
        "fraud_title": "‰∏çÊ≠£Ê§úÁü•",
        "fraud_score": "‰∏çÊ≠£„É™„Çπ„ÇØ„Çπ„Ç≥„Ç¢",
        "fraud_indicators": "Ê§úÂá∫ÊåáÊ®ô",
        "fraud_reasoning": "Âà§Êñ≠Ê†πÊã†",
        "evidence_title": "Ë®ºÊã†",
        "hazards_title": "Ê§úÂá∫„Åï„Çå„ÅüÂç±Èô∫",
        "causal": "Âõ†ÊûúÂàÜÊûê",
        "action": "Êé®Â•®„Ç¢„ÇØ„Ç∑„Éß„É≥",
        "processing_time": "Âá¶ÁêÜÊôÇÈñì",
        "stage_timings": "„Çπ„ÉÜ„Éº„Ç∏Âà•Âá¶ÁêÜÊôÇÈñì",
        "report_title": "„É¨„Éù„Éº„Éà",
        "download_json": "JSON„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ",
        "download_html": "HTML„É¨„Éù„Éº„Éà„ÉÄ„Ç¶„É≥„É≠„Éº„Éâ",
        "no_results": "ÊüªÂÆöÁµêÊûú„Åå„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ„Éë„Ç§„Éó„É©„Ç§„É≥„ÇíÂÆüË°å„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
        "pipeline_success": "„Éë„Ç§„Éó„É©„Ç§„É≥Ê≠£Â∏∏ÂÆå‰∫Ü!",
        "pipeline_failed": "„Éë„Ç§„Éó„É©„Ç§„É≥Â§±Êïó",
        "ground_truth": "Ê≠£Ëß£„Éá„Éº„Çø",
        "gt_match": "‰∏ÄËá¥",
        "gt_mismatch": "‰∏ç‰∏ÄËá¥",
        "ego_vehicle": "Ëá™Ëªä",
        "other_vehicle": "Áõ∏ÊâãËªä",
        "shared": "ÂèåÊñπÈÅéÂ§±",
        "vlm_backend": "VLM„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ",
        "conformal_alpha": "Conformal Alpha",
        "overview": "Ê¶ÇË¶Å",
        "details": "Ë©≥Á¥∞",
        "comparison": "Ê≠£Ëß£„Éá„Éº„ÇøÊØîËºÉ",
        "backend_label": "„Éê„ÉÉ„ÇØ„Ç®„É≥„Éâ",
        "backend_mock": "MockÔºàGPU‰∏çË¶ÅÔºâ",
        "backend_real": "RealÔºàQwen2.5-VL-7BÔºâ",
        "backend_replay": "„É™„Éó„É¨„Ç§ÔºàJSONË™≠ËæºÔºâ",
        "replay_select": "ÁµêÊûú„Éï„Ç°„Ç§„É´„ÇíÈÅ∏Êäû",
        "replay_upload": "„Åæ„Åü„ÅØJSON„Éï„Ç°„Ç§„É´„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ",
        "replay_loaded": "„Éï„Ç°„Ç§„É´„Åã„ÇâÁµêÊûú„ÇíË™≠„ÅøËæº„Åø„Åæ„Åó„Åü",
        "replay_no_files": "reports/ „Å´ÁµêÊûú„Éï„Ç°„Ç§„É´„ÅåË¶ã„Å§„Åã„Çä„Åæ„Åõ„Çì",
        "gpu_not_available": "GPU„ÅåÂà©Áî®„Åß„Åç„Åæ„Åõ„Çì„ÄÇCUDAÂØæÂøúPyTorch„Çí„Ç§„É≥„Çπ„Éà„Éº„É´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
        "gpu_info": "GPU",
        "real_warning": "Real VLMÊé®Ë´ñ„Å´„ÅØGPUÔºàVRAM 14GB‰ª•‰∏äÔºâ„ÅåÂøÖË¶Å„Åß„Åô„ÄÇÂãïÁîª1Êú¨„ÅÇ„Åü„ÇäÁ¥Ñ2„Äú3ÂàÜ„ÄÇ",
        "replay_source": "„ÇΩ„Éº„Çπ",
        "mode_batch": "„Éê„ÉÉ„ÉÅÁµêÊûú",
        "batch_title": "„Éê„ÉÉ„ÉÅÂá¶ÁêÜÁµêÊûú",
        "batch_upload_prompt": "batch_report.json„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ",
        "batch_loaded": "„Éê„ÉÉ„ÉÅ„É¨„Éù„Éº„Éà„ÇíË™≠„ÅøËæº„Åø„Åæ„Åó„Åü",
        "batch_total_videos": "ÂãïÁîªÁ∑èÊï∞",
        "batch_successful": "ÊàêÂäü",
        "batch_failed": "Â§±Êïó",
        "batch_severity_dist": "ÈáçÂ§ßÂ∫¶ÂàÜÂ∏É",
        "batch_avg_confidence": "Âπ≥ÂùáÁ¢∫‰ø°Â∫¶",
        "batch_avg_time": "Âπ≥ÂùáÂá¶ÁêÜÊôÇÈñì/ÂãïÁîª",
        "batch_total_time": "Á∑èÂá¶ÁêÜÊôÇÈñì",
        "batch_video_id": "ÂãïÁîªID",
        "batch_file": "„Éï„Ç°„Ç§„É´",
        "batch_severity": "ÈáçÂ§ßÂ∫¶",
        "batch_confidence": "Á¢∫‰ø°Â∫¶",
        "batch_processing_time": "ÊôÇÈñì (Áßí)",
        "batch_status": "„Çπ„ÉÜ„Éº„Çø„Çπ",
        "batch_no_report": "„Éê„ÉÉ„ÉÅ„É¨„Éù„Éº„Éà„ÅåË™≠„ÅøËæº„Åæ„Çå„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇbatch_report.json„Çí„Ç¢„ÉÉ„Éó„É≠„Éº„Éâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
        "batch_per_video": "ÂãïÁîªÂà•ÁµêÊûú",
        "batch_aggregate": "ÈõÜË®àÁµ±Ë®à",
        "accuracy_label": "ÈáçÂ§ßÂ∫¶Á≤æÂ∫¶",
        "accuracy_note": "Êã°Âºµ10ÂãïÁîª„ÉÜ„Çπ„Éà„Çπ„Ç§„Éº„Éà„Åß9/10",
    },
}

DEMO_VIDEOS = {
    "collision": {
        "en": "Rear-end Collision (HIGH severity)",
        "ja": "ËøΩÁ™Å‰∫ãÊïÖ (ÈáçÂ§ßÂ∫¶: HIGH)",
        "gt_severity": "HIGH",
        "gt_fault": 100.0,
        "gt_fraud": 0.0,
        "gt_scenario": "rear_end",
    },
    "near_miss": {
        "en": "Pedestrian Near-Miss (MEDIUM severity)",
        "ja": "Ê≠©Ë°åËÄÖ„Éã„Ç¢„Éü„Çπ (ÈáçÂ§ßÂ∫¶: MEDIUM)",
        "gt_severity": "MEDIUM",
        "gt_fault": 0.0,
        "gt_fraud": 0.0,
        "gt_scenario": "pedestrian_avoidance",
    },
    "normal": {
        "en": "Normal Driving (NONE severity)",
        "ja": "ÈÄöÂ∏∏Ëµ∞Ë°å (ÈáçÂ§ßÂ∫¶: NONE)",
        "gt_severity": "NONE",
        "gt_fault": 0.0,
        "gt_fraud": 0.0,
        "gt_scenario": "normal_driving",
    },
}


# ---------------------------------------------------------------------------
# Helper functions
# ---------------------------------------------------------------------------


def t(key: str) -> str:
    """Get translation for current language."""
    lang = st.session_state.get("lang", "ja")
    return TRANSLATIONS.get(lang, TRANSLATIONS["en"]).get(key, key)


def severity_badge(severity: str) -> str:
    """Create HTML badge for severity level."""
    color = SEVERITY_COLORS.get(severity, "#6B7280")
    return (
        f'<span style="background-color:{color};color:white;padding:4px 12px;'
        f'border-radius:9999px;font-weight:bold;font-size:0.9em;">{severity}</span>'
    )


def gauge_metric(value: float, max_val: float, label: str, color: str = "#3B82F6") -> str:
    """Create a simple gauge-style metric HTML."""
    pct = min(value / max_val * 100, 100) if max_val > 0 else 0
    return f"""
    <div style="margin:8px 0;">
        <div style="display:flex;justify-content:space-between;margin-bottom:4px;">
            <span style="font-weight:600;">{label}</span>
            <span style="font-weight:700;color:{color};">{value:.1f}{"%" if max_val == 100 else ""}</span>
        </div>
        <div style="background:#E5E7EB;border-radius:9999px;height:12px;overflow:hidden;">
            <div style="background:{color};height:100%;width:{pct}%;border-radius:9999px;
                        transition:width 0.5s ease;"></div>
        </div>
    </div>
    """


def check_gpu() -> dict:
    """Check GPU availability for real VLM inference."""
    try:
        import torch

        if torch.cuda.is_available():
            name = torch.cuda.get_device_name(0)
            vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            return {"available": True, "name": name, "vram_gb": round(vram, 1)}
        return {"available": False, "name": None, "vram_gb": 0}
    except ImportError:
        return {"available": False, "name": None, "vram_gb": 0}


def run_pipeline(video_path: str, video_id: str, backend: str = "mock") -> dict:
    """Run the insurance pipeline on a video.

    Args:
        video_path: Path to video file
        video_id: Video identifier
        backend: "mock" or "real"
    """
    output_dir = tempfile.mkdtemp(prefix="insurance_dashboard_")

    config = PipelineConfig(
        output_dir=output_dir,
        log_level="WARNING",
        parallel_workers=1,
        enable_conformal=True,
        enable_transcription=False,
        continue_on_error=True,
    )

    if backend == "real":
        config.cosmos.backend = CosmosBackend.QWEN25VL
    else:
        config.cosmos.backend = CosmosBackend.MOCK

    pipeline = InsurancePipeline(config)

    if backend == "mock":
        pipeline.signal_fuser = None
        pipeline.cosmos_client = None

    result = pipeline.process_video(str(video_path), video_id=video_id)

    return {
        "video_id": result.video_id,
        "success": result.success,
        "error_message": result.error_message,
        "assessments": result.assessments or [],
        "processing_time_sec": result.processing_time_sec,
        "stage_timings": result.stage_timings or {},
        "output_json_path": result.output_json_path,
        "output_html_path": result.output_html_path,
        "backend": backend,
    }


def load_replay_json(json_path: str | Path) -> dict | None:
    """Load benchmark/replay results from JSON file.

    Supports two formats:
    - Benchmark format: {"videos": {"collision": {"actual": {...}}}, "summary": {...}}
    - Pipeline format: [{"severity": ..., "confidence": ...}]
    """
    from insurance_mvp.cosmos.schema import ClaimAssessment, FaultAssessment, FraudRisk

    try:
        with open(json_path, encoding="utf-8") as f:
            data = json.load(f)
    except (json.JSONDecodeError, OSError):
        return None

    # Format 1: Benchmark results (real_benchmark.json)
    if isinstance(data, dict) and "videos" in data:
        all_assessments = []
        for vid_name, vid_data in data["videos"].items():
            if not vid_data.get("success", False):
                continue
            actual = vid_data.get("actual", {})
            try:
                assessment = ClaimAssessment(
                    severity=actual.get("severity", "LOW"),
                    confidence=actual.get("confidence", 0.0),
                    prediction_set=set(actual.get("prediction_set", ["LOW"])),
                    review_priority=actual.get("review_priority", "STANDARD"),
                    fault_assessment=FaultAssessment(
                        fault_ratio=actual.get("fault_ratio", 50.0),
                        reasoning=actual.get("fault_reasoning", "From replay"),
                        scenario_type=actual.get("scenario_type", "unknown"),
                    ),
                    fraud_risk=FraudRisk(
                        risk_score=actual.get("fraud_score", 0.0),
                        indicators=actual.get("fraud_indicators", []),
                        reasoning="From replay",
                    ),
                    hazards=[],
                    evidence=[],
                    causal_reasoning=actual.get("causal_reasoning", "Loaded from replay"),
                    recommended_action=actual.get("recommended_action", "REVIEW"),
                    video_id=vid_name,
                    processing_time_sec=vid_data.get("inference_time_sec", 0.0),
                )
                all_assessments.append(assessment)
            except Exception:
                continue

        if not all_assessments:
            return None

        return {
            "video_id": "replay",
            "success": True,
            "error_message": None,
            "assessments": all_assessments,
            "processing_time_sec": data.get("summary", {}).get("total_inference_time_sec", 0.0),
            "stage_timings": {},
            "output_json_path": None,
            "output_html_path": None,
            "backend": f"replay ({data.get('backend', 'unknown')})",
            "replay_source": str(json_path),
            "replay_metadata": {
                "model": data.get("model", "unknown"),
                "timestamp": data.get("timestamp", "unknown"),
                "model_load_time": data.get("model_load_time_sec", 0),
                "summary": data.get("summary", {}),
            },
        }

    return None


def load_batch_report(json_path: str | Path) -> dict | None:
    """Load a batch_report.json produced by scripts/batch_process.py.

    Expected structure:
        {
          "timestamp": "...",
          "config": {...},
          "summary": {"total_videos": N, "successful": N, "failed": N,
                       "severity_distribution": {...}, "avg_confidence": float,
                       "total_processing_time_sec": float, "avg_time_per_video_sec": float},
          "gpu_memory": {...},
          "videos": [{"video_id": str, "file": str, "success": bool,
                       "processing_time_sec": float, "severity": str|null,
                       "confidence": float|null, "fault_ratio": float|null,
                       "fraud_score": float|null, "num_assessments": int, "error": str|null}, ...]
        }
    """
    try:
        with open(json_path, encoding="utf-8") as f:
            data = json.load(f)
    except (json.JSONDecodeError, OSError):
        return None

    if isinstance(data, dict) and "videos" in data and isinstance(data["videos"], list):
        return data
    return None


# ---------------------------------------------------------------------------
# Page config
# ---------------------------------------------------------------------------

st.set_page_config(
    page_title="Insurance Claim Assessment",
    page_icon="üöó",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Custom CSS
st.markdown(
    """
<style>
    .main-header {
        background: linear-gradient(135deg, #1E3A5F 0%, #2563EB 100%);
        color: white;
        padding: 24px 32px;
        border-radius: 12px;
        margin-bottom: 24px;
    }
    .main-header h1 { color: white; margin: 0; font-size: 2em; }
    .main-header p { color: #BFDBFE; margin: 4px 0 0; }
    .metric-card {
        background: white;
        border: 1px solid #E5E7EB;
        border-radius: 12px;
        padding: 20px;
        box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    .metric-card h3 { margin: 0 0 12px; color: #374151; font-size: 1.1em; }
    .severity-high { border-left: 4px solid #DC2626; }
    .severity-medium { border-left: 4px solid #F59E0B; }
    .severity-low { border-left: 4px solid #3B82F6; }
    .severity-none { border-left: 4px solid #10B981; }
    .gt-match { color: #10B981; font-weight: bold; }
    .gt-mismatch { color: #DC2626; font-weight: bold; }
    .stage-timing { display: flex; justify-content: space-between; padding: 4px 0; }
    div[data-testid="stMetricValue"] { font-size: 1.8em; }
</style>
""",
    unsafe_allow_html=True,
)

# ---------------------------------------------------------------------------
# Session state initialization
# ---------------------------------------------------------------------------

if "lang" not in st.session_state:
    st.session_state.lang = "ja"
if "results" not in st.session_state:
    st.session_state.results = None
if "selected_video" not in st.session_state:
    st.session_state.selected_video = None
if "backend" not in st.session_state:
    st.session_state.backend = "mock"

# ---------------------------------------------------------------------------
# Sidebar
# ---------------------------------------------------------------------------

with st.sidebar:
    st.markdown(f"### {t('sidebar_title')}")

    lang = st.radio(t("lang_label"), ["ja", "en"], index=0, format_func=lambda x: "Êó•Êú¨Ë™û" if x == "ja" else "English")
    st.session_state.lang = lang

    st.divider()

    # Backend selector
    backend_options = {
        "mock": t("backend_mock"),
        "real": t("backend_real"),
        "replay": t("backend_replay"),
        "batch": t("mode_batch"),
    }
    backend = st.radio(
        t("backend_label"),
        list(backend_options.keys()),
        format_func=lambda x: backend_options[x],
    )
    st.session_state.backend = backend

    # GPU info for real backend
    if backend == "real":
        gpu_info = check_gpu()
        if gpu_info["available"]:
            st.success(f"üü¢ {gpu_info['name']} ({gpu_info['vram_gb']} GB)")
        else:
            st.error(t("gpu_not_available"))
        st.caption(t("real_warning"))

    st.divider()

    # Mode selector (not needed for replay/batch)
    if backend not in ("replay", "batch"):
        mode = st.radio(t("mode_label"), ["demo", "upload"], format_func=lambda x: t(f"mode_{x}"))
    else:
        mode = backend

    st.divider()

    st.markdown(f"**{t('vlm_backend')}**: {backend_options[backend]}")
    st.markdown(f"**{t('conformal_alpha')}**: 0.10 (90% CI)")

    st.divider()
    st.caption(f"{t('accuracy_label')}: 90% ({t('accuracy_note')})")
    st.caption("Insurance MVP v0.2.0")
    st.caption("Powered by Qwen2.5-VL + Conformal Prediction")

# ---------------------------------------------------------------------------
# Header
# ---------------------------------------------------------------------------

st.markdown(
    f"""
<div class="main-header">
    <h1>üöó {t("title")}</h1>
    <p>{t("subtitle")}</p>
</div>
""",
    unsafe_allow_html=True,
)

# ---------------------------------------------------------------------------
# Video selection
# ---------------------------------------------------------------------------

video_path = None
video_id = None
gt_data = None

if mode == "replay":
    # ------------------------------------------------------------------
    # Replay mode: load JSON results from previous GPU runs
    # ------------------------------------------------------------------
    st.markdown("### " + t("backend_replay"))

    # Find existing result files (demo_assets first, then reports/)
    demo_assets_dir = PROJECT_ROOT / "insurance_mvp" / "demo_assets"
    reports_dir = PROJECT_ROOT / "reports"
    result_files = []
    if demo_assets_dir.exists():
        result_files.extend(sorted(demo_assets_dir.glob("*.json"), reverse=True))
    if reports_dir.exists():
        result_files.extend(sorted(reports_dir.glob("*.json"), reverse=True))

    replay_source = None

    if result_files:
        file_options = {str(f): f.name for f in result_files}
        selected_file = st.selectbox(
            t("replay_select"), list(file_options.keys()), format_func=lambda x: file_options[x]
        )
        if selected_file:
            replay_source = selected_file

    # Also allow upload
    uploaded_json = st.file_uploader(t("replay_upload"), type=["json"])
    if uploaded_json:
        tmp_json = Path(tempfile.mkdtemp()) / uploaded_json.name
        tmp_json.write_bytes(uploaded_json.read())
        replay_source = str(tmp_json)

    if replay_source:
        replay_data = load_replay_json(replay_source)
        if replay_data:
            st.session_state.results = replay_data
            st.success(f"{t('replay_loaded')}: {Path(replay_source).name}")

            # Show replay metadata
            meta = replay_data.get("replay_metadata", {})
            if meta:
                meta_col1, meta_col2, meta_col3 = st.columns(3)
                with meta_col1:
                    st.metric("Model", meta.get("model", "unknown"))
                with meta_col2:
                    summary = meta.get("summary", {})
                    st.metric("Severity Accuracy", f"{summary.get('severity_accuracy', 0)}%")
                with meta_col3:
                    st.metric("Inference Time", f"{summary.get('mean_inference_time_sec', 0):.1f}s/video")

            # Show per-video results selector for replay
            if replay_data.get("assessments") and len(replay_data["assessments"]) > 1:
                video_names = [a.video_id for a in replay_data["assessments"]]
                selected_replay = st.selectbox("Select video result", video_names)
                if selected_replay:
                    # Filter to selected video
                    sel_assessment = [a for a in replay_data["assessments"] if a.video_id == selected_replay]
                    if sel_assessment:
                        replay_data_filtered = dict(replay_data)
                        replay_data_filtered["assessments"] = sel_assessment
                        replay_data_filtered["video_id"] = selected_replay
                        st.session_state.results = replay_data_filtered
                        # Set GT data if it's a demo video
                        if selected_replay in DEMO_VIDEOS:
                            gt_data = DEMO_VIDEOS[selected_replay]
                            video_id = selected_replay
        else:
            st.warning("Could not parse JSON file. Expected benchmark or pipeline format.")

elif mode == "batch":
    # ------------------------------------------------------------------
    # Batch mode: load batch_report.json from batch_process.py
    # ------------------------------------------------------------------
    st.markdown("### " + t("batch_title"))

    # Find existing batch report files
    batch_result_files = []
    results_dir = PROJECT_ROOT / "results"
    if results_dir.exists():
        batch_result_files.extend(sorted(results_dir.glob("**/batch_report.json"), reverse=True))
    reports_dir = PROJECT_ROOT / "reports"
    if reports_dir.exists():
        batch_result_files.extend(sorted(reports_dir.glob("**/batch_report.json"), reverse=True))

    batch_source = None

    if batch_result_files:
        file_options = {str(f): str(f.relative_to(PROJECT_ROOT)) for f in batch_result_files}
        selected_batch_file = st.selectbox(
            t("replay_select"), list(file_options.keys()), format_func=lambda x: file_options[x]
        )
        if selected_batch_file:
            batch_source = selected_batch_file

    # Also allow upload
    uploaded_batch = st.file_uploader(t("batch_upload_prompt"), type=["json"])
    if uploaded_batch:
        tmp_batch = Path(tempfile.mkdtemp()) / uploaded_batch.name
        tmp_batch.write_bytes(uploaded_batch.read())
        batch_source = str(tmp_batch)

    if batch_source:
        batch_data = load_batch_report(batch_source)
        if batch_data:
            st.success(f"{t('batch_loaded')}: {Path(batch_source).name}")
            summary = batch_data.get("summary", {})
            videos_list = batch_data.get("videos", [])

            # -- Aggregate stats row --
            st.markdown(f"#### {t('batch_aggregate')}")
            agg_cols = st.columns(5)
            with agg_cols[0]:
                st.metric(t("batch_total_videos"), summary.get("total_videos", len(videos_list)))
            with agg_cols[1]:
                st.metric(t("batch_successful"), summary.get("successful", 0))
            with agg_cols[2]:
                st.metric(t("batch_failed"), summary.get("failed", 0))
            with agg_cols[3]:
                avg_conf = summary.get("avg_confidence")
                st.metric(t("batch_avg_confidence"), f"{avg_conf:.1%}" if avg_conf is not None else "N/A")
            with agg_cols[4]:
                st.metric(t("batch_avg_time"), f"{summary.get('avg_time_per_video_sec', 0):.1f}s")

            # -- Severity distribution --
            sev_dist = summary.get("severity_distribution", {})
            if sev_dist:
                st.markdown(f"**{t('batch_severity_dist')}**")
                dist_cols = st.columns(len(sev_dist))
                for i, (sev_label, count) in enumerate(sorted(sev_dist.items())):
                    with dist_cols[i]:
                        color = SEVERITY_COLORS.get(sev_label, "#6B7280")
                        emoji = SEVERITY_EMOJI.get(sev_label, "")
                        st.markdown(
                            f'<div style="text-align:center;padding:8px;border-left:4px solid {color};'
                            f'background:#F9FAFB;border-radius:8px;">'
                            f'<div style="font-size:1.6em;font-weight:700;color:{color};">{count}</div>'
                            f'<div style="color:#6B7280;">{emoji} {sev_label}</div></div>',
                            unsafe_allow_html=True,
                        )

            # -- Per-video results table --
            st.markdown("---")
            st.markdown(f"#### {t('batch_per_video')}")

            # Build table data
            table_rows = []
            for v in videos_list:
                status = "OK" if v.get("success") else "FAIL"
                sev_val = v.get("severity") or "-"
                conf_val = f"{v['confidence']:.1%}" if v.get("confidence") is not None else "-"
                time_val = f"{v.get('processing_time_sec', 0):.2f}"
                error_val = v.get("error") or ""
                table_rows.append({
                    t("batch_video_id"): v.get("video_id", ""),
                    t("batch_file"): v.get("file", ""),
                    t("batch_status"): status,
                    t("batch_severity"): sev_val,
                    t("batch_confidence"): conf_val,
                    t("batch_processing_time"): time_val,
                })

            if table_rows:
                st.dataframe(table_rows, use_container_width=True, hide_index=True)

            # -- Config & metadata --
            with st.expander("Batch Config / Metadata"):
                config_info = batch_data.get("config", {})
                st.json(config_info)
                gpu_info_batch = batch_data.get("gpu_memory", {})
                if gpu_info_batch:
                    st.markdown("**GPU Memory**")
                    st.json(gpu_info_batch)

            # -- Total processing time footer --
            st.markdown("---")
            st.metric(
                t("batch_total_time"),
                f"{summary.get('total_processing_time_sec', 0):.1f}s",
            )
        else:
            st.warning("Could not parse batch report. Expected batch_process.py output format.")

elif mode == "demo":
    demo_options = {k: v[lang] for k, v in DEMO_VIDEOS.items()}
    selected = st.selectbox(t("select_video"), list(demo_options.keys()), format_func=lambda k: demo_options[k])

    if selected:
        video_file = DEMO_DIR / f"{selected}.mp4"
        if video_file.exists():
            video_path = str(video_file)
            video_id = selected
            gt_data = DEMO_VIDEOS[selected]

            col1, col2 = st.columns([2, 1])
            with col1:
                st.video(str(video_file))
            with col2:
                st.markdown(f"#### {t('ground_truth')}")
                gt_sev = gt_data["gt_severity"]
                st.markdown(f"**{t('severity')}**: {SEVERITY_EMOJI.get(gt_sev, '')} {gt_sev}")
                st.markdown(f"**{t('fault_ratio')}**: {gt_data['gt_fault']:.0f}%")
                st.markdown(f"**{t('scenario')}**: {gt_data['gt_scenario']}")
                st.markdown(f"**{t('fraud_score')}**: {gt_data['gt_fraud']:.1f}")
        else:
            st.warning(f"Demo video not found: {video_file}")
else:
    uploaded = st.file_uploader(t("upload_prompt"), type=["mp4", "avi", "mov"])
    if uploaded:
        tmp = Path(tempfile.mkdtemp()) / uploaded.name
        tmp.write_bytes(uploaded.read())
        video_path = str(tmp)
        video_id = tmp.stem
        st.video(str(tmp))

# ---------------------------------------------------------------------------
# Run pipeline
# ---------------------------------------------------------------------------

if video_path and mode not in ("replay", "batch"):
    # Check if real backend is available
    can_run = True
    if backend == "real":
        gpu_info = check_gpu()
        if not gpu_info["available"]:
            st.error(t("gpu_not_available"))
            can_run = False

    if can_run and st.button(t("run_button"), type="primary", use_container_width=True):
        with st.spinner(t("running")):
            start = time.time()
            results = run_pipeline(video_path, video_id, backend=backend)
            elapsed = time.time() - start

        st.session_state.results = results
        st.session_state.selected_video = video_id

        if results["success"]:
            st.success(f"{t('pipeline_success')} ({elapsed:.1f}s) [{backend_options[backend]}]")
        else:
            st.error(f"{t('pipeline_failed')}: {results.get('error_message', 'unknown')}")

# ---------------------------------------------------------------------------
# Results display
# ---------------------------------------------------------------------------

results = st.session_state.results
if results and results.get("assessments"):
    assessments = results["assessments"]
    primary = assessments[0] if assessments else None

    if primary is None:
        st.info(t("no_results"))
    else:
        st.markdown(f"## {t('results_title')}")

        tab_overview, tab_details, tab_comparison = st.tabs([t("overview"), t("details"), t("comparison")])

        # ---------------------------------------------------------------
        # Tab 1: Overview
        # ---------------------------------------------------------------
        with tab_overview:
            col1, col2, col3, col4 = st.columns(4)

            sev = primary.severity
            with col1:
                st.markdown(
                    f"""<div class="metric-card severity-{sev.lower()}">
                    <h3>{t("severity")}</h3>
                    <div style="font-size:2.2em;font-weight:800;color:{SEVERITY_COLORS.get(sev, "#6B7280")};">
                        {SEVERITY_EMOJI.get(sev, "")} {sev}
                    </div>
                    <div style="color:#6B7280;margin-top:4px;">
                        {t("confidence")}: {primary.confidence:.0%}
                    </div>
                </div>""",
                    unsafe_allow_html=True,
                )

            with col2:
                fault = primary.fault_assessment.fault_ratio
                fault_color = "#DC2626" if fault >= 75 else "#F59E0B" if fault >= 25 else "#10B981"
                at_fault = t("ego_vehicle") if fault >= 75 else t("other_vehicle") if fault <= 25 else t("shared")
                st.markdown(
                    f"""<div class="metric-card">
                    <h3>{t("fault_title")}</h3>
                    <div style="font-size:2.2em;font-weight:800;color:{fault_color};">
                        {fault:.0f}%
                    </div>
                    <div style="color:#6B7280;margin-top:4px;">{at_fault}</div>
                </div>""",
                    unsafe_allow_html=True,
                )

            with col3:
                fraud = primary.fraud_risk.risk_score
                fraud_color = "#DC2626" if fraud >= 0.65 else "#F59E0B" if fraud >= 0.4 else "#10B981"
                fraud_label = "HIGH" if fraud >= 0.65 else "MEDIUM" if fraud >= 0.4 else "LOW"
                st.markdown(
                    f"""<div class="metric-card">
                    <h3>{t("fraud_title")}</h3>
                    <div style="font-size:2.2em;font-weight:800;color:{fraud_color};">
                        {fraud:.2f}
                    </div>
                    <div style="color:#6B7280;margin-top:4px;">{fraud_label} Risk</div>
                </div>""",
                    unsafe_allow_html=True,
                )

            with col4:
                priority = primary.review_priority
                p_color = PRIORITY_COLORS.get(priority, "#6B7280")
                p_label_map = {
                    "URGENT": "Á∑äÊÄ•" if lang == "ja" else "URGENT",
                    "STANDARD": "Ê®ôÊ∫ñ" if lang == "ja" else "STANDARD",
                    "LOW_PRIORITY": "‰Ωé" if lang == "ja" else "LOW",
                }
                st.markdown(
                    f"""<div class="metric-card">
                    <h3>{t("review_priority")}</h3>
                    <div style="font-size:2.2em;font-weight:800;color:{p_color};">
                        {p_label_map.get(priority, priority)}
                    </div>
                    <div style="color:#6B7280;margin-top:4px;">{t("action")}: {primary.recommended_action}</div>
                </div>""",
                    unsafe_allow_html=True,
                )

            # Prediction set
            st.markdown("---")
            pred_set = primary.prediction_set
            pred_badges = " ".join(
                f'<span style="background:{SEVERITY_COLORS.get(s, "#6B7280")};color:white;'
                f'padding:3px 10px;border-radius:9999px;margin:2px;font-size:0.85em;">{s}</span>'
                for s in sorted(
                    pred_set,
                    key=lambda x: ["NONE", "LOW", "MEDIUM", "HIGH"].index(x)
                    if x in ["NONE", "LOW", "MEDIUM", "HIGH"]
                    else 99,
                )
            )
            st.markdown(
                f"**{t('prediction_set')}**: {pred_badges}",
                unsafe_allow_html=True,
            )

            # Causal reasoning
            st.markdown("---")
            st.markdown(f"#### {t('causal')}")
            st.info(primary.causal_reasoning)

            # Processing time
            st.markdown("---")
            timing_col1, timing_col2 = st.columns(2)
            with timing_col1:
                st.metric(t("processing_time"), f"{results['processing_time_sec']:.2f}s")
            with timing_col2:
                if results.get("stage_timings"):
                    st.markdown(f"**{t('stage_timings')}**")
                    for stage, dur in results["stage_timings"].items():
                        st.markdown(f"- {stage}: `{dur:.3f}s`")

        # ---------------------------------------------------------------
        # Tab 2: Details
        # ---------------------------------------------------------------
        with tab_details:
            # Fault details
            st.markdown(f"### {t('fault_title')}")
            fa = primary.fault_assessment
            st.markdown(
                gauge_metric(fa.fault_ratio, 100, t("fault_ratio"), fault_color),
                unsafe_allow_html=True,
            )
            st.markdown(f"**{t('scenario')}**: `{fa.scenario_type}`")
            if fa.traffic_signal:
                st.markdown(f"**Traffic Signal**: `{fa.traffic_signal}`")
            if fa.right_of_way:
                st.markdown(f"**Right of Way**: `{fa.right_of_way}`")
            st.markdown(f"**{t('reasoning')}**")
            st.text(fa.reasoning)
            if fa.applicable_rules:
                st.markdown(f"**{t('rules')}**")
                for rule in fa.applicable_rules:
                    st.markdown(f"- {rule}")

            st.divider()

            # Fraud details
            st.markdown(f"### {t('fraud_title')}")
            fr = primary.fraud_risk
            st.markdown(
                gauge_metric(fr.risk_score * 100, 100, t("fraud_score"), fraud_color),
                unsafe_allow_html=True,
            )
            if fr.indicators:
                st.markdown(f"**{t('fraud_indicators')}**")
                for ind in fr.indicators:
                    st.markdown(f"- ‚ö†Ô∏è {ind}")
            else:
                st.success("No fraud indicators detected" if lang == "en" else "‰∏çÊ≠£ÊåáÊ®ô„ÅØÊ§úÂá∫„Åï„Çå„Åæ„Åõ„Çì„Åß„Åó„Åü")
            st.markdown(f"**{t('fraud_reasoning')}**")
            st.text(fr.reasoning)

            st.divider()

            # Evidence
            if primary.evidence:
                st.markdown(f"### {t('evidence_title')}")
                for ev in primary.evidence:
                    st.markdown(f"- **{ev.timestamp_sec:.1f}s**: {ev.description}")

            # Hazards
            if primary.hazards:
                st.markdown(f"### {t('hazards_title')}")
                for hz in primary.hazards:
                    st.markdown(
                        f"- **{hz.type}** at {hz.timestamp_sec:.1f}s - Actors: {', '.join(hz.actors)} ({hz.spatial_relation})"
                    )

            st.divider()

            # Raw JSON
            st.markdown("### Raw Assessment Data")
            with st.expander("JSON"):
                st.json(primary.model_dump(mode="json", exclude={"timestamp"}))

        # ---------------------------------------------------------------
        # Tab 3: Ground Truth Comparison
        # ---------------------------------------------------------------
        with tab_comparison:
            if gt_data:
                st.markdown(f"### {t('comparison')}")

                comp_col1, comp_col2, comp_col3 = st.columns(3)

                with comp_col1:
                    st.markdown("**Field**")
                    st.markdown(t("severity"))
                    st.markdown(t("fault_ratio"))
                    st.markdown(t("fraud_score"))
                    st.markdown(t("scenario"))

                with comp_col2:
                    st.markdown(f"**{t('ground_truth')}**")
                    st.markdown(f"{SEVERITY_EMOJI.get(gt_data['gt_severity'], '')} {gt_data['gt_severity']}")
                    st.markdown(f"{gt_data['gt_fault']:.0f}%")
                    st.markdown(f"{gt_data['gt_fraud']:.1f}")
                    st.markdown(f"`{gt_data['gt_scenario']}`")

                with comp_col3:
                    st.markdown("**AI Assessment**")

                    # Severity: exact match, borderline (distance=1), or mismatch
                    sev_match = primary.severity == gt_data["gt_severity"]
                    sev_order = {"NONE": 0, "LOW": 1, "MEDIUM": 2, "HIGH": 3}
                    sev_dist = abs(sev_order.get(primary.severity, -1) - sev_order.get(gt_data["gt_severity"], -1))
                    sev_in_pred_set = gt_data["gt_severity"] in primary.prediction_set

                    if sev_match:
                        sev_icon = "‚úÖ"
                    elif sev_dist == 1:
                        sev_icon = "üü°"  # Borderline
                    else:
                        sev_icon = "‚ùå"
                    sev_note = f" {primary.severity} {sev_icon}"
                    if not sev_match and sev_dist == 1:
                        borderline_label = "Borderline" if lang == "en" else "Â¢ÉÁïåÂÄ§"
                        sev_note += f' <span style="color:#F59E0B;font-size:0.85em;">({borderline_label})</span>'
                    if not sev_match and sev_in_pred_set:
                        covered_label = "Covered by 90% CI" if lang == "en" else "90% CI„Åß„Ç´„Éê„Éº"
                        sev_note += f' <span style="color:#3B82F6;font-size:0.85em;">({covered_label})</span>'
                    st.markdown(
                        f"{SEVERITY_EMOJI.get(primary.severity, '')}{sev_note}",
                        unsafe_allow_html=True,
                    )

                    fault_match = abs(primary.fault_assessment.fault_ratio - gt_data["gt_fault"]) < 20
                    st.markdown(f"{primary.fault_assessment.fault_ratio:.0f}% {'‚úÖ' if fault_match else '‚ùå'}")
                    fraud_match = abs(primary.fraud_risk.risk_score - gt_data["gt_fraud"]) < 0.3
                    st.markdown(f"{primary.fraud_risk.risk_score:.1f} {'‚úÖ' if fraud_match else '‚ùå'}")
                    st.markdown(f"`{primary.fault_assessment.scenario_type}`")

                # Accuracy summary (borderline counts as partial pass)
                st.divider()
                sev_score = 1.0 if sev_match else (0.5 if sev_dist == 1 else 0.0)
                checks_score = sev_score + (1.0 if fault_match else 0.0) + (1.0 if fraud_match else 0.0)
                total = 3
                display_score = f"{checks_score:.1f}" if checks_score != int(checks_score) else f"{int(checks_score)}"
                acc_color = "#10B981" if checks_score >= 2.5 else "#F59E0B" if checks_score >= 1.5 else "#DC2626"
                acc_label = "Accuracy Score" if lang == "en" else "Á≤æÂ∫¶„Çπ„Ç≥„Ç¢"
                st.markdown(
                    f"""<div style="text-align:center;padding:16px;background:#F9FAFB;border-radius:12px;">
                    <div style="font-size:2em;font-weight:800;color:{acc_color};">{display_score}/{total}</div>
                    <div style="color:#6B7280;">{acc_label}</div>
                </div>""",
                    unsafe_allow_html=True,
                )

                # Conformal coverage note
                if not sev_match and sev_in_pred_set:
                    coverage_note = (
                        "The ground truth severity is within the model's 90% confidence interval (prediction set). "
                        "This indicates the model's uncertainty quantification is well-calibrated."
                        if lang == "en"
                        else "Ê≠£Ëß£„ÅÆÈáçÂ§ßÂ∫¶„ÅØ„É¢„Éá„É´„ÅÆ90%‰ø°È†ºÂå∫ÈñìÔºà‰∫àÊ∏¨„Çª„ÉÉ„ÉàÔºâÂÜÖ„Å´Âê´„Åæ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ"
                        "„Åì„Çå„ÅØ„É¢„Éá„É´„ÅÆ‰∏çÁ¢∫ÂÆüÊÄßÂÆöÈáèÂåñ„ÅåÈÅ©Âàá„Å´Ê†°Ê≠£„Åï„Çå„Å¶„ÅÑ„Çã„Åì„Å®„ÇíÁ§∫„Åó„Åæ„Åô„ÄÇ"
                    )
                    st.info(coverage_note)
            else:
                st.info(
                    "Ground truth comparison is only available for demo videos."
                    if lang == "en"
                    else "Ê≠£Ëß£„Éá„Éº„ÇøÊØîËºÉ„ÅØ„Éá„É¢ÂãïÁîª„ÅÆ„ÅøÂà©Áî®ÂèØËÉΩ„Åß„Åô„ÄÇ"
                )

        # ---------------------------------------------------------------
        # Download buttons
        # ---------------------------------------------------------------
        st.markdown("---")
        dl_col1, dl_col2 = st.columns(2)

        with dl_col1:
            json_data = json.dumps(
                [a.model_dump(mode="json", exclude={"timestamp"}) for a in assessments],
                indent=2,
                ensure_ascii=False,
            )
            st.download_button(
                t("download_json"),
                data=json_data,
                file_name=f"assessment_{video_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json",
                mime="application/json",
                use_container_width=True,
            )

        with dl_col2:
            if results.get("output_html_path") and Path(results["output_html_path"]).exists():
                html_content = Path(results["output_html_path"]).read_text(encoding="utf-8")
                st.download_button(
                    t("download_html"),
                    data=html_content,
                    file_name=f"report_{video_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.html",
                    mime="text/html",
                    use_container_width=True,
                )

elif not results:
    st.markdown(
        f"""<div style="text-align:center;padding:60px 20px;color:#9CA3AF;">
        <div style="font-size:4em;margin-bottom:16px;">üé•</div>
        <p style="font-size:1.2em;">{t("no_results")}</p>
    </div>""",
        unsafe_allow_html=True,
    )
